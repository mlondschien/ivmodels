{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Using Geographic Variation in College Proximity to Estimate the Return to Schooling\n","\n","Card (1993) estimates the causal effect of length of education on hourly wages.\n","\n","We start by loading the data."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from io import BytesIO\n","from zipfile import ZipFile\n","\n","import numpy as np\n","import pandas as pd\n","import requests\n","\n","url = \"https://davidcard.berkeley.edu/data_sets/proximity.zip\"\n","content = requests.get(url).content\n","\n","# From code_bk.txt in the zip file\n","colspec = {\n","    \"id\": (1, 5),  # sequential id runs from 1 to 5225\n","    \"nearc2\": (7, 7),  # grew up near 2-yr college\n","    \"nearc4\": (10, 10),  # grew up near 4-yr college\n","    \"nearc4a\": (12, 13),  # grew up near 4-yr public college\n","    \"nearc4b\": (15, 16),  # grew up near 4-yr priv college\n","    \"ed76\": (18, 19),  # educ in 1976\n","    \"ed66\": (21, 22),  # educ in 1966\n","    \"age76\": (24, 25),  # age in 1976\n","    \"daded\": (27, 31),  # dads education missing=avg\n","    \"nodaded\": (33, 33),  # 1 if dad ed imputed\n","    \"momed\": (35, 39),  # moms education\n","    \"nomomed\": (41, 41),  # 1 if mom ed imputed\n","    \"weight\": (43, 54),  # nls weight for 1976 cross-section\n","    \"momdad14\": (56, 56),  # 1 if live with mom and dad age 14\n","    \"sinmom14\": (58, 58),  # lived with single mom age 14\n","    \"step14\": (60, 60),  # lived step parent age 14\n","    \"reg661\": (62, 62),  # dummy for region=1 in 1966\n","    \"reg662\": (64, 64),  # dummy for region=2 in 1966\n","    \"reg663\": (66, 66),  # dummy for region=3 in 1966\n","    \"reg664\": (68, 68),\n","    \"reg665\": (70, 70),\n","    \"reg666\": (72, 72),\n","    \"reg667\": (74, 74),\n","    \"reg668\": (76, 76),\n","    \"reg669\": (78, 78),  # dummy for region=9 in 1966\n","    \"south66\": (80, 80),  # lived in south in 1966\n","    \"work76\": (82, 82),  # worked in 1976\n","    \"work78\": (84, 84),  # worked in 1978\n","    \"lwage76\": (86, 97),  # log wage (outliers trimmed) 1976\n","    \"lwage78\": (99, 110),  # log wage in 1978 outliers trimmed\n","    \"famed\": (112, 112),  # mom-dad education class 1-9\n","    \"black\": (114, 114),  # 1 if black\n","    \"smsa76r\": (116, 116),  # in smsa in 1976\n","    \"smsa78r\": (118, 118),  # in smsa in 1978\n","    \"reg76r\": (120, 120),  # in south in 1976\n","    \"reg78r\": (122, 122),  # in south in 1978\n","    \"reg80r\": (124, 124),  # in south in 1980\n","    \"smsa66r\": (126, 126),  # in smsa in 1966\n","    \"wage76\": (128, 132),  # raw wage cents per hour 1976\n","    \"wage78\": (134, 138),\n","    \"wage80\": (140, 144),\n","    \"noint78\": (146, 146),  # 1 if noninterview in 78\n","    \"noint80\": (148, 148),\n","    \"enroll76\": (150, 150),  # 1 if enrolled in 76\n","    \"enroll78\": (152, 152),\n","    \"enroll80\": (154, 154),\n","    \"kww\": (156, 157),  # the kww score\n","    \"iq\": (159, 161),  # a normed iq score\n","    \"marsta76\": (163, 163),  # mar status in 1976 1=married, sp. present\n","    \"marsta78\": (165, 165),\n","    \"marsta80\": (167, 167),\n","    \"libcrd14\": (169, 169),  # 1 if lib card in home age 14\n","}\n","\n","with ZipFile(BytesIO(content)).open(\"nls.dat\") as file:\n","    df = pd.read_fwf(\n","        file,\n","        names=colspec.keys(),\n","        # pandas expects [from, to[ values, starting at 0\n","        colspecs=[(f - 1, t) for (f, t) in colspec.values()],\n","        na_values=\".\",\n","    )\n","\n","df = df[lambda x: x[\"lwage76\"].notna()].set_index(\"id\")\n","\n","# construct potential experience and its square\n","df[\"exp76\"] = df[\"age76\"] - df[\"ed76\"] - 6\n","df[\"exp762\"] = df[\"exp76\"] ** 2\n","df[\"age762\"] = df[\"age76\"] ** 2\n","\n","df[\"f1\"] = df[\"famed\"].eq(1).astype(\"float\")  # mom and dad both > 12 yrs ed\n","df[\"f2\"] = df[\"famed\"].eq(2).astype(\"float\")  # mom&dad >=12 and not both exactly 12\n","df[\"f3\"] = df[\"famed\"].eq(3).astype(\"float\")  # mom=dad=12\n","df[\"f4\"] = df[\"famed\"].eq(4).astype(\"float\")  # mom >=12 and dad missing\n","df[\"f5\"] = df[\"famed\"].eq(5).astype(\"float\")  # father >=12 and mom not in f1-f4\n","df[\"f6\"] = df[\"famed\"].eq(6).astype(\"float\")  # mom>=12 and dad nonmissing\n","df[\"f7\"] = df[\"famed\"].eq(7).astype(\"float\")  # mom and dad both >=9\n","df[\"f8\"] = df[\"famed\"].eq(8).astype(\"float\")  # mom and dad both nonmissing\n","\n","indicators = [\"black\", \"smsa66r\", \"smsa76r\", \"reg76r\"]\n","# exclude reg669, as sum(reg661, ..., reg669) = 1\n","indicators += [f\"reg66{i}\" for i in range(1, 9)]\n","\n","family = [\"daded\", \"momed\", \"nodaded\", \"nomomed\", \"famed\", \"momdad14\", \"sinmom14\"]\n","fs = [f\"f{i}\" for i in range(1, 9)]  # exclude f9 as sum(f1, ..., f9) = 1\n","family += fs\n","\n","X = df[[\"ed76\", \"exp76\", \"exp762\"]]  # endogenous\n","y = df[\"lwage76\"]  # outcome\n","C = df[family + indicators]  # included exogenous variables\n","Z = df[[\"nearc4a\", \"nearc4b\", \"nearc2\", \"age76\", \"age762\"]]  # instruments"]},{"cell_type":"markdown","metadata":{},"source":["We then estimate the causal effect of education on wages using the Two-Stage Least-Squares (TSLS) and Limited Information Maximum Likelihood (LIML) estimators."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["intercept    4.680112\n","ed76         0.144954\n","exp76        0.061604\n","exp762      -0.001196\n","Name: coefficients, dtype: float64\n","intercept    3.060812\n","ed76         0.172352\n","exp76        0.051571\n","exp762      -0.000713\n","Name: coefficients, dtype: float64\n"]}],"source":["from ivmodels.models import KClass\n","\n","tsls = KClass(kappa=\"tsls\").fit(Z=Z, X=X, y=y, C=C)\n","print(tsls.named_coefs_[:4])\n","\n","liml = KClass(kappa=\"liml\").fit(Z=Z, X=X, y=y, C=C)\n","print(liml.named_coefs_[:4])"]},{"cell_type":"markdown","metadata":{},"source":["To influence policy, inference for causal effect estimates is crucial.\n","Test statistics, p-values, and confidence sets can be computed using the tests in `ivmodels.tests`."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["statistic=284.905, p_value=0\n"]}],"source":["\n","from ivmodels.tests import wald_test\n","\n","statistic, p_value = wald_test(Z=Z, X=X, C=C, y=y, beta=np.zeros(X.shape[1]))\n","print(f\"{statistic=:.3f}, {p_value=:.3g}\")"]},{"cell_type":"markdown","metadata":{},"source":["That is, the joint causal effect of education, experience, and experience squared is highly significant using the Wald test.\n","Subvector inference for individual components of the causal effect is easier to interpret."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["statistic=10.529, p_value=0.00118\n","95.0% confidence set: [0.057, 0.233]\n","99.0% confidence set: [0.030, 0.260]\n","99.9% confidence set: [-0.002, 0.292]\n"]}],"source":["from ivmodels.confidence_set import ConfidenceSet\n","from ivmodels.tests import inverse_wald_test\n","\n","statistic, p_value = wald_test(Z=Z, X=df[[\"ed76\"]], W=df[[\"exp76\", \"exp762\"]], C=C, y=y, beta=np.zeros(1))\n","print(f\"{statistic=:.3f}, {p_value=:.3g}\")\n","\n","for alpha in [0.05, 0.01, 0.001]:\n","    quadric = inverse_wald_test(Z=Z, X=df[[\"ed76\"]], W=df[[\"exp76\", \"exp762\"]], C=C, y=y, alpha=alpha)\n","    confidence_set = ConfidenceSet.from_quadric(quadric)\n","    print(f\"{100 * (1 - alpha)}% confidence set: {confidence_set:.3f}\")"]},{"cell_type":"markdown","metadata":{},"source":["Test statistics, p-values, and confidence sets can be computed for multiple features using the `summary` method of `KClass`.\n","This also displays results of Anderson's (1951) test of reduced rank (a multivariate extension of the first-stage F-test) and the LIML (weak instrument robust) variant of the J-statistic."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Summary based on the wald test.\n","\n","         estimate  statistic   p-value              conf. set\n","ed76        0.145      10.53  0.001175       [0.0574, 0.2325]\n","exp76      0.0616      6.635  0.009997      [0.01473, 0.1085]\n","exp762  -0.001196      1.029    0.3104  [-0.003506, 0.001115]\n","\n","Endogenous model statistic: 284.9, p-value: <1e-16\n","(Multivariate) F-statistic: 15.47, p-value: 0.001454\n","J-statistic (LIML): 4.246, p-value: 0.1197\n"]}],"source":["features = [\"ed76\", \"exp76\", \"exp762\"]\n","print(tsls.summary(X=X, Z=Z, C=C, y=y, feature_names=features))"]},{"cell_type":"markdown","metadata":{},"source":["Anderson's test statistic of reduced rank is of the order 10, suggesting that instruments might be weak. The `ivmodels` package implements three weak-instrument robust tests: the (subvector) Anderson-Rubin test (Anderson and Rubin, 1949 and Guggenberger et al., 2012), the (subvector) conditional likelihood-ratio test (Moreira, 2003 and Kleibergen, 2021), and the (subvector) Lagrange multiplier test (Kleibergen, 2002 and Londschien et al., 2024). \n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Summary based on the anderson-rubin test.\n","\n","          estimate  statistic   p-value              conf. set\n","ed76        0.1724      5.027  0.001748      [0.08205, 0.3557]\n","exp76      0.05157      2.093   0.09876    [-0.02839, 0.09739]\n","exp762  -0.0007127      1.495    0.2138  [-0.002968, 0.003186]\n","\n","Endogenous model statistic: 66.33, p-value: <1e-16\n","(Multivariate) F-statistic: 15.47, p-value: 0.001454\n","J-statistic (LIML): 4.246, p-value: 0.1197\n","\n","Summary based on the conditional likelihood-ratio test.\n","\n","          estimate  statistic   p-value              conf. set\n","ed76        0.1724      10.84  0.002516      [0.07273, 0.3987]\n","exp76      0.05157      2.034    0.1785     [-0.04522, 0.1018]\n","exp762  -0.0007127     0.2379    0.6444  [-0.003198, 0.004002]\n","\n","Endogenous model statistic: 327.4, p-value: <1e-16\n","(Multivariate) F-statistic: 15.47, p-value: 0.001454\n","J-statistic (LIML): 4.246, p-value: 0.1197\n","\n","Summary based on the lagrange multiplier test.\n","\n","          estimate  statistic  p-value                                      conf. set\n","ed76        0.1724      5.739  0.01659         [-0.6013, -0.05795] U [0.06078, 0.472]\n","exp76      0.05157      1.648   0.1992           [-0.06781, 0.1025] U [0.116, 0.3515]\n","exp762  -0.0007127     0.2043   0.6513  [-0.01522, -0.003838] U [-0.003228, 0.005095]\n","\n","Endogenous model statistic: 324.9, p-value: <1e-16\n","(Multivariate) F-statistic: 15.47, p-value: 0.001454\n","J-statistic (LIML): 4.246, p-value: 0.1197\n","\n"]}],"source":["for test in [\"anderson-rubin\", \"conditional likelihood-ratio\", \"lagrange multiplier\"]:\n","    print(liml.summary(X=X, Z=Z, C=C, y=y, feature_names=features, test=test))\n","    print(\"\")"]},{"cell_type":"markdown","metadata":{},"source":["The causal effect of education on wages is still significant at level 0.01 for the weak-instrument robust tests."]}],"metadata":{"kernelspec":{"display_name":"ivmodels","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":2}
